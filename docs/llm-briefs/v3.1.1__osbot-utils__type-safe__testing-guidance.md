# Type_Safe Testing Guidance for LLMs

- **version**: v3.1.1
- **updated**: 12th September 2025
- **companion document**: Type_Safe & Python Formatting Guide for LLMs v3.1.1

## Overview

This document provides comprehensive testing patterns for Type_Safe-based services using the OSBot framework. It is designed to be consumed by LLMs when generating tests for Type_Safe code, while also being readable by humans who need to understand the testing philosophy and rationale behind these patterns.

Use this guide in conjunction with the "Type_Safe & Python Formatting Guide for LLMs" for complete coverage of Type_Safe patterns. While the main guide covers the Type_Safe framework itself, this document focuses specifically on how to write robust, maintainable tests that validate both the type safety guarantees and business logic of Type_Safe applications.

## Quick Reference: Key Testing Areas

This guide covers seven critical areas of Type_Safe testing:

1. **Core Testing Patterns** - Context managers, .obj() comparisons, inline comments, and why these patterns create better tests
2. **Type Safety Validation** - Auto-conversion behavior, Safe type testing, collection transformations, and understanding Type_Safe's unique runtime guarantees
3. **Performance Optimization** - setUpClass patterns, shared test objects, resource management, and achieving 10-100x speed improvements
4. **Advanced Testing with __ Class** - __SKIP__, .contains(), .diff(), .excluding(), .merge() and how these make tests more maintainable
5. **Component-Specific Testing** - Schemas, Services, FastAPI routes, Cache/Storage with real-world patterns
6. **Bug Documentation Pattern** - Writing passing tests for bugs, regression suite management, and preserving institutional knowledge
7. **Safe Type Edge Cases** - LLM types, Git/GitHub types, Crypto types, Enum/Literal support, and domain-specific validation

---

## Part 1: Core Testing Patterns

### File Structure and Naming

The test file structure must mirror the source code structure exactly. This convention makes it trivial to find tests for any given source file and ensures nothing is missed during refactoring:

```python
# Source file: mgraph_ai_service_llms/service/llms/LLM__Service.py
# Test file:   tests/unit/service/llms/test_LLM__Service.py

# Always prefix test files with 'test_'
# Mirror source directory structure under tests/unit/
```

This pattern ensures that:
- Developers can instantly locate tests for any source file
- CI/CD systems can automatically discover and run all tests
- Test coverage tools can accurately map tests to source code
- Refactoring tools can maintain the relationship between source and tests

### Documentation Style - NEVER Use Docstrings

**CRITICAL**: In Type_Safe testing, we NEVER use Python docstrings. This is not a stylistic preference but a fundamental requirement that maintains the visual pattern recognition that makes Type_Safe code readable and debuggable.

Following Type_Safe's visual alignment philosophy (detailed in the main guide), all documentation must be inline comments aligned at the end of lines. This maintains the visual "lanes" that make code structure immediately apparent:

```python
# ✓ CORRECT - Inline comments maintain visual alignment
def test__init__(self):                                   # Test auto-initialization of Schema__Persona
    with Schema__Persona() as _:
        assert type(_)         is Schema__Persona
        assert base_classes(_) == [Type_Safe, object]
        assert type(_.id)      is Safe_Id                 # Test all fields are initialized
        assert type(_.name)    is Safe_Str__Text
        
        # Complex assertions get block comments above
        # but NEVER docstrings that break the visual flow

# ✗ WRONG - Docstrings break visual patterns and hide structure
def test__init__(self):
    """Test auto-initialization of Schema__Persona"""    # NO! Breaks alignment
    with Schema__Persona() as _:
        assert type(_) is Schema__Persona
```

The reason this matters is that Type_Safe code is designed to be scanned visually. When you can follow the alignment columns, bugs become immediately visible as misalignments. Docstrings create vertical walls of text that break this pattern recognition, making the code harder to debug and maintain.

### Context Manager Pattern with '_'

The context manager pattern with underscore is not just a convention - it's the standard throughout Type_Safe testing for several important reasons:

1. **Resource Management**: Context managers ensure proper cleanup even when tests fail
2. **Readability**: The underscore `_` is a Python convention for "the current thing we're working with"
3. **Consistency**: Every Type_Safe object supports context managers, making this pattern universal
4. **Debugging**: Stack traces are cleaner when using context managers

```python
# ✓ PREFERRED - Context manager with underscore
def test__init__(self):
    with Schema__Order() as _:
        assert type(_.id)     is Safe_Str__OrderId
        assert _.items        == {}                      # Note: Type_Safe__Dict shows as {}
        assert _.status       == "pending"

# ✗ AVOID - Direct variable assignment misses benefits
def test__init__(self):
    order = Schema__Order()  # No automatic cleanup
    assert type(order.id) is Safe_Str__OrderId
    # If this fails, no cleanup happens
```

### Method Naming Convention

Maintaining a one-to-one mapping between class methods and test methods is crucial for test coverage and maintenance. This pattern makes it immediately obvious if any method lacks tests:

```python
class An_Class(Type_Safe):
    def method_1(self): pass
    def method_2(self, var_1): pass

class test_An_Class(TestCase):
    def test__init__(self): pass                          # Always test initialization
    def test_method_1(self): pass                         # Direct mapping to method_1
    def test_method_2(self): pass                         # Direct mapping to method_2
    def test_method_2__handle_bad_data(self): pass        # Variation with __ suffix
    def test_method_2__with_special_chars(self): pass     # Another variation
    def test__integration_scenario(self): pass            # Cross-method scenario with __ prefix
```

This naming convention provides several benefits:
- **Complete Coverage**: Easy to see if any method lacks a test
- **Organized Variations**: Related tests grouped by method name  
- **Clear Scenarios**: Double underscore prefix for non-method-specific tests
- **Navigation**: IDEs can jump between method and its tests
- **Maintenance**: When a method changes, you know exactly which tests to update

---

## Part 2: The .obj() Method and __ Class

### Understanding .obj() and Why It Matters

The `.obj()` method is one of Type_Safe's most powerful testing features, converting Type_Safe objects to `__` (double underscore) instances for comprehensive comparisons. This isn't just syntactic sugar - it fundamentally changes how we write and maintain tests.

Traditional testing requires multiple assertions to verify object state, leading to verbose, fragile tests that break when new fields are added. The `.obj()` method solves this by enabling single-assertion verification of complete object state:

```python
from osbot_utils.testing.__ import __

def test__init__(self):
    with Schema__Order() as _:
        # ✗ OLD WAY - Multiple assertions, hard to maintain
        # assert _.id is not None
        # assert _.items == {}
        # assert _.total == 0.00
        # assert _.status == "pending"
        # assert _.tracking is None
        # Must update every time schema changes!
        
        # ✓ NEW WAY - Single comprehensive comparison
        assert _.obj() == __(id          = _.id            ,  # Auto-generated values
                             items       = {}               ,  # Empty Type_Safe__Dict
                             total       = 0.00             ,  # Zero values
                             status      = "pending"        ,  # Explicit defaults
                             tracking    = None             )  # Nullable fields
```

This approach provides:
- **Completeness**: Ensures all fields are tested
- **Maintainability**: Changes are localized to one place
- **Readability**: Object state is visible at a glance
- **Debugging**: Differences are shown clearly in test output

### Advanced __ Class Features

The `__` class returned by `.obj()` is not just a simple namespace - it's a sophisticated testing tool with multiple capabilities designed specifically for Type_Safe testing scenarios.

#### Handling Dynamic Values with __SKIP__

One of the biggest challenges in testing is dealing with auto-generated values like IDs, timestamps, and random tokens. These values are different every time, making tests brittle if you try to assert specific values. The `__SKIP__` marker solves this elegantly:

```python
from osbot_utils.testing.__ import __, __SKIP__

def test__with_dynamic_fields(self):
    with Schema__Order() as order:
        order.items = {'product-1': {'name': 'laptop', 'qty': 1}}
        
        # Without __SKIP__, this test would fail due to random ID/timestamps
        assert order.obj() == __(order_id   = __SKIP__      ,  # Skip auto-generated ID
                                 items      = {'product-1': __(name='laptop', qty=1)},
                                 created_at = __SKIP__      ,  # Skip timestamp
                                 updated_at = __SKIP__      )  # Skip timestamp
```

**When to use __SKIP__:**
- Auto-generated IDs (Safe_Id, UUIDs, Random_Guid)
- Timestamps (created_at, updated_at, timestamp fields)
- Random values or tokens
- External API response IDs you don't control
- Any value that's non-deterministic

**When NOT to use __SKIP__:**
- Values you explicitly set
- Business logic results
- Values that should be deterministic
- Core data that defines correctness

#### Partial Matching with .contains()

Sometimes you only care about specific fields in an object, especially when dealing with large API responses or when testing specific functionality. The `.contains()` method enables subset matching without asserting the entire structure:

```python
def test__partial_matching(self):
    with Schema__User() as user:
        user.name = 'Alice'
        user.age = 30
        user.email = 'alice@test.com'
        user.tags = ['admin', 'developer']
        
        # Only verify critical fields without checking everything
        assert user.obj().contains(__(name='Alice', age=30))
        assert not user.obj().contains(__(name='Bob'))
        
        # Especially useful for API responses
        response = self.api.create_user(user_data)
        # Don't assert entire response, just what matters
        assert response.obj().contains(__(status='success', user_id=__SKIP__))
```

**Use cases for .contains():**
- Verify critical fields without full object comparison
- Test API responses where you only care about specific data
- Validate nested structures partially
- Quick assertions in integration tests
- Focus on business logic results while ignoring metadata

#### Debugging with .diff()

When tests fail, especially with large objects, understanding what's different can be challenging. The `.diff()` method provides detailed difference reports that make debugging much easier:

```python
def test__debugging_differences(self):
    with Schema__User() as user1:
        user1.email = 'alice@old.com'
        user1.age = 25
        
    with Schema__User() as user2:
        user2.email = 'alice@new.com'
        user2.age = 26
        
    # Get detailed difference report
    diff = user1.obj().diff(user2.obj())
    assert diff == {
        'email': {'actual': 'alice@old.com', 'expected': 'alice@new.com'},
        'age':   {'actual': 25, 'expected': 26}
    }
    
    # Use in test failures for better debugging
    if user1.obj() != user2.obj():
        print(f"Objects differ: {user1.obj().diff(user2.obj())}")
        # This shows EXACTLY what's different, not just "assertion failed"
```

This is invaluable when:
- Tests fail in CI/CD where you can't debug interactively
- Comparing complex nested structures
- Understanding why round-trip serialization failed
- Tracking down subtle state changes

#### Creating Variations with .merge()

Test data management is a constant challenge. You need variations of test objects for different scenarios, but copying and modifying creates maintenance nightmares. The `.merge()` method solves this elegantly:

```python
def test__test_data_variations(self):
    # Create base test data
    with Schema__Request() as base_request:
        base_request.model = 'gpt-4'
        base_request.prompt = 'Hello'
        base_request.temperature = 0.7
        
    base_obj = base_request.obj()
    
    # Create variations without duplication
    high_temp_request = base_obj.merge(temperature=0.9, top_p=0.95)
    low_temp_request  = base_obj.merge(temperature=0.1, max_tokens=50)
    streaming_request = base_obj.merge(stream=True, temperature=0.5)
    
    # Each variation preserves base values except what's overridden
    assert high_temp_request.model == 'gpt-4'  # Base preserved
    assert high_temp_request.temperature == 0.9  # Override applied
```

This pattern enables:
- DRY (Don't Repeat Yourself) test data
- Clear documentation of what's different in each test case
- Easy addition of new test scenarios
- Reduced maintenance when base data changes

### When to Use .obj() vs .json()

Understanding when to use each method is crucial for writing maintainable tests:

```python
# Use .obj() for Type_Safe objects with valid Python identifiers
# Better for internal state verification
assert _.obj() == __(field1=value1, field2=value2)

# Use .json() for API responses or special characters in keys
# Required when keys aren't valid Python identifiers
assert response.json() == {'content-type': 'application/json'}  # Hyphen in key
```

**Use .obj() when:**
- Testing Type_Safe object initialization
- Comparing internal state
- All keys are valid Python identifiers
- You need __ class features (__SKIP__, .contains(), etc.)

**Use .json() when:**
- Testing API responses
- Keys contain special characters (hyphens, spaces)
- Comparing with external JSON data
- Serialization format matters

---

## Part 3: Performance Optimization

### The Critical Importance of setUpClass vs setUp

One of the most impactful decisions in test design is choosing between `setUpClass` and `setUp`. This choice can literally make your test suite 10-100x faster or slower. Understanding when to use each is crucial for maintaining a fast, responsive test suite that developers will actually run.

The fundamental principle: **Expensive operations should happen ONCE per test class, not once per test method.**

```python
class test_Heavy_Service(TestCase):
    
    @classmethod
    def setUpClass(cls):                                  # ONE-TIME expensive setup
        # These operations might take 1-5 seconds EACH
        setup__service_fast_api_test_objs()               # LocalStack setup (2-3s)
        cls.s3_client = S3()                              # S3 connection (1s)
        cls.service = Heavy_Service()                     # Service initialization (0.5s)
        cls.service.connect_to_database()                 # Database connection (1s)
        
        # Total setup time: ~5 seconds ONCE for entire test class
        
    @classmethod  
    def tearDownClass(cls):                               # ONE-TIME cleanup
        # Clean up in reverse order of creation
        cls.service.disconnect()
        cls.s3_client.bucket_delete(cls.test_bucket)
        
    def setUp(self):                                      # PER-TEST lightweight setup
        # Only millisecond operations here
        self.test_id = Random_Guid()                      # <1ms
        self.temp_file = f"test_{self.test_id}.json"      # <1ms
        
    def tearDown(self):                                   # PER-TEST cleanup
        # Quick state resets only
        self.service.clear_cache()                        # <10ms
```

**The Math**: If you have 20 test methods:
- Using `setUp` for expensive operations: 5 seconds × 20 tests = 100 seconds
- Using `setUpClass`: 5 seconds × 1 = 5 seconds
- **Speed improvement: 20x faster!**

**Use setUpClass for:**
- Service initialization
- Database/S3/API connections
- Loading configuration files
- Creating test buckets/tables
- Setting up LocalStack
- Any operation taking >100ms
- Resources that can be safely shared between tests

**Use setUp for:**
- Test-specific IDs/timestamps
- Temporary files/data
- Mutable state that must be fresh
- Quick variable assignments (<10ms)
- Test isolation requirements

### Test Atomicity and Shared Resources

If you find yourself needing fresh instances for every test, this indicates potential design issues in your code, not the test:

```python
# ⚠️ BAD SIGN - Service has hidden state
class test_Problematic_Service(TestCase):
    def setUp(self):
        self.service = Problematic_Service()  # Required fresh for each test
        
    def test_1(self):
        self.service.process("data")
        
    def test_2(self):
        # This fails if using shared instance - indicates hidden state!
        self.service.process("data")

# ✓ GOOD - Service is properly atomic
class test_Atomic_Service(TestCase):
    @classmethod
    def setUpClass(cls):
        cls.service = Atomic_Service()  # Can be safely shared
        
    def tearDown(self):
        self.service.reset()  # Explicit state reset if needed
```

Services that can't be shared between tests often have:
- Hidden state accumulation
- Memory leaks
- Improper connection pooling
- Cache pollution
- Threading issues

### Shared Test Objects Pattern

For integration tests, especially those involving FastAPI and LocalStack, creating a shared test infrastructure dramatically improves performance. Instead of each test class creating its own FastAPI app and LocalStack instance, create them ONCE for the entire test suite:

```python
# tests/unit/Service__Fast_API__Test_Objs.py
from osbot_utils.type_safe.Type_Safe import Type_Safe

class Service__Fast_API__Test_Objs(Type_Safe):
    fast_api         : Service__Fast_API     = None
    fast_api__app    : FastAPI               = None
    fast_api__client : TestClient            = None
    localstack_setup : LocalStack__Setup     = None
    setup_completed  : bool                  = False

service_fast_api_test_objs = Service__Fast_API__Test_Objs()  # Singleton

def setup__service_fast_api_test_objs():
    with service_fast_api_test_objs as _:
        if _.setup_completed is False:                       # Only setup once
            _.localstack_setup = LocalStack__Setup().setup() # ~3 seconds
            _.fast_api         = Service__Fast_API().setup() # ~1 second
            _.fast_api__app    = _.fast_api.app()           
            _.fast_api__client = _.fast_api.client()        
            _.setup_completed  = True                        # Never run again
    return service_fast_api_test_objs
```

This pattern provides:
- **Massive performance gains**: Setup happens once for hundreds of tests
- **Consistency**: All tests use the same configured environment
- **Resource efficiency**: One LocalStack instance instead of dozens
- **Faster feedback**: Developers run tests more often when they're fast

Using shared test objects:

```python
class test_Routes__LLMs__client(TestCase):
    @classmethod
    def setUpClass(cls):
        with setup__service_fast_api_test_objs() as _:
            cls.client = _.fast_api__client              # Reuse shared client
            # First test class: 4 seconds setup
            # Subsequent test classes: <0.1 seconds!
```

---

## Part 4: Type Safety Validation Testing

### Understanding Type_Safe's Auto-Conversion Philosophy

Type_Safe's automatic type conversion is one of its most powerful features, but also one of the most misunderstood. Unlike traditional type systems that simply reject incorrect types, Type_Safe attempts to convert values to their correct types when possible. This is a **deliberate design choice** that makes Type_Safe practical for real-world applications where data comes from JSON, forms, databases, and APIs.

The key insight: **In production, data rarely arrives in the exact type you need.** Type_Safe acknowledges this reality and handles it gracefully:

```python
def test_type_auto_conversion(self):                     # Test Type_Safe's automatic conversion
    with Schema__Persona() as _:
        # String to Safe_Id - sanitizes special characters
        _.id = "raw-string!@£"
        assert type(_.id) is Safe_Id                     # Auto-converted
        assert _.id == 'raw-string___'                   # Special chars replaced
        
        # Integer to Safe_Str - converts via str()
        _.name = 123
        assert type(_.name) is Safe_Str__Text            # Auto-converted
        assert _.name == '123'                           # Integer stringified
        
        # This is a FEATURE for real-world data handling:
        # - JSON deserializes everything as strings/numbers
        # - Form data arrives as strings
        # - Databases may return different numeric types
        # Type_Safe handles all of these gracefully
```

### The Conversion vs Validation Distinction

Understanding when Type_Safe converts vs when it validates vs when it raises errors is crucial for writing correct tests:

```python
def test_type_enforcement_scenarios(self):    
    with Schema__User() as _:
        # SCENARIO 1: Successful conversion + validation
        #             in this case _.age is of type Safe_UInt
        _.age = "25"                                     # String to uint
        assert _.age == 25                               # Converted successfully
        
        # SCENARIO 2: Successful conversion, failed validation
        with pytest.raises(ValueError):                  # Note: ValueError, not TypeError!
            _.age = -5                                   # Converts to uint, but negative invalid
            
        # SCENARIO 3: Impossible conversion
        with pytest.raises(TypeError):                   # Only TypeError when can't convert
            _.name = {'dict': 'value'}                   # Cannot convert dict to string
```

The hierarchy is:
1. **Try conversion** (string→int, int→string, etc.)
2. **If conversion succeeds, validate** (bounds, format, etc.)
3. **ValueError if validation fails** (wrong format, out of bounds)
4. **TypeError only if conversion impossible** (incompatible types)

This distinction matters because:
- **ValueError** means the data could be converted but isn't valid
- **TypeError** means the data structure is fundamentally wrong
- Different error types require different fixes in production

### Collection Type Transformation

One of Type_Safe's security features is that ALL collections become Type_Safe variants. This prevents the shared mutable state bugs that plague Python applications:

```python
def test_collection_type_conversion(self):
    class Schema__Test(Type_Safe):
        regular_dict : dict                              # Becomes Type_Safe__Dict
        typed_list   : List[str]                         # Becomes Type_Safe__List
        typed_set    : Set[int]                          # Becomes Type_Safe__Set
        
    with Schema__Test() as _:
        # Verify Type_Safe variants are used
        assert type(_.regular_dict) is Type_Safe__Dict
        assert type(_.typed_list)   is Type_Safe__List
        assert type(_.typed_set)    is Type_Safe__Set
        
        # These are NOT raw Python collections
        assert type(_.regular_dict) is not dict
        assert type(_.typed_list)   is not list
        
        # Why this matters: Each instance gets its OWN collection
        obj1 = Schema__Test()
        obj2 = Schema__Test()
        obj1.typed_list.append("secret")
        assert "secret" not in obj2.typed_list           # No shared state!
```

This transformation:
- **Prevents data leaks** between instances
- **Enables type checking** on collection operations
- **Provides consistent behavior** across all Type_Safe objects
- **Eliminates entire categories of bugs** related to mutable defaults

### Testing Strategy for Type Conversion

When testing Type_Safe classes, always test both the "happy path" and the conversion path:

```python
def test_comprehensive_type_validation(self):
    # Test 1: Pre-converted Safe types (ideal scenario)
    with Schema__Config() as _:
        _.api_key = Safe_Str__API_Key("KEY-123-ABC")
        assert _.api_key == "KEY-123-ABC"
        
    # Test 2: Raw values (realistic scenario)
    with Schema__Config(api_key="KEY-123-ABC") as _:
        assert _.api_key == "KEY-123-ABC"                # Should handle conversion
        
    # Test 3: Edge cases
    with pytest.raises(ValueError):                      # Invalid format
        Schema__Config(api_key="invalid-key-format")
        
    # Test 4: Type incompatibility  
    with pytest.raises(TypeError):                       # Cannot convert
        Schema__Config(api_key=['list', 'of', 'keys'])
```

---

## Part 5: Testing Safe Type Variants

### Domain-Specific Safe Types and Their Purpose

Type_Safe includes dozens of specialized Safe types for different domains. Each type encodes specific validation rules, size limits, and sanitization logic. Understanding how to test these properly is crucial because they're your first line of defense against security vulnerabilities and data corruption.

### LLM-Specific Safe Types

The LLM Safe types encode the specific constraints of different LLM APIs and use cases. These aren't arbitrary - they match real API limits and prevent common integration failures:

```python
from osbot_utils.type_safe.primitives.domains.llm.safe_str.Safe_Str__LLM__Message__System import Safe_Str__LLM__Message__System
from osbot_utils.type_safe.primitives.domains.llm.safe_str.Safe_Str__LLM__Message__User import Safe_Str__LLM__Message__User
from osbot_utils.type_safe.primitives.domains.llm.safe_str.Safe_Str__LLM__Message__Assistant import Safe_Str__LLM__Message__Assistant

def test_llm_message_types(self):
    class Schema__Conversation(Type_Safe):
        system_prompt   : Safe_Str__LLM__Message__System
        user_message    : Safe_Str__LLM__Message__User
        assistant_reply : Safe_Str__LLM__Message__Assistant
        
    with Schema__Conversation() as _:
        # System prompts have tighter limits (4KB)
        # because they're included in EVERY request
        _.system_prompt = "Configure behavior"
        assert len(_.system_prompt) <= 4096              
        
        # User messages can use full context window (32KB)
        large_message = "Hello " * 5000  # ~30KB
        _.user_message = large_message
        assert len(_.user_message) <= 32768
        
        # Control characters can break JSON serialization
        _.assistant_reply = "Response\x00with\x01control"
        assert '\x00' not in _.assistant_reply           # Filtered out
        assert '\x01' not in _.assistant_reply
        
        # These limits prevent:
        # - API rejection due to oversized requests
        # - JSON serialization failures
        # - Token limit exceeded errors that cost money
```

### Git/GitHub Safe Types

These types enforce the actual validation rules used by Git and GitHub, preventing errors when integrating with version control:

```python
from osbot_utils.type_safe.primitives.domains.git.github.safe_str.Safe_Str__GitHub__Repo import Safe_Str__GitHub__Repo
from osbot_utils.type_safe.primitives.domains.git.safe_str.Safe_Str__Git__Branch import Safe_Str__Git__Branch
from osbot_utils.type_safe.primitives.domains.git.safe_str.Safe_Str__Git__Tag import Safe_Str__Git__Tag

def test_github_types(self):
    class Schema__Repository(Type_Safe):
        repo   : Safe_Str__GitHub__Repo
        branch : Safe_Str__Git__Branch
        tag    : Safe_Str__Git__Tag
        
    with Schema__Repository() as _:
        # GitHub repo format is strict: owner/name
        _.repo = "octocat/Hello-World"
        assert _.repo.repo_owner == "octocat"            # Parsed automatically
        assert _.repo.repo_name == "Hello-World"         
        
        # This parsing enables:
        # - Automatic GitHub API URL construction
        # - Owner-based permission checks
        # - Repository grouping in UIs
        
        # Git has specific branch name rules
        with pytest.raises(ValueError):
            _.branch = "-invalid-branch"                 # Cannot start with dash
        with pytest.raises(ValueError):
            _.branch = "branch..name"                    # Cannot contain ..
        with pytest.raises(ValueError):
            _.branch = "branch~name"                     # Cannot contain ~
            
        # These rules prevent:
        # - Git command failures
        # - Security issues with branch names
        # - GitHub API rejections
```

### Cryptographic Safe Types

Cryptographic types enforce exact length requirements and character sets required by cryptographic operations:

```python
from osbot_utils.type_safe.primitives.domains.cryptography.safe_str.Safe_Str__SHA1 import Safe_Str__SHA1
from osbot_utils.type_safe.primitives.domains.cryptography.safe_str.Safe_Str__NaCl__Public_Key import Safe_Str__NaCl__Public_Key
from osbot_utils.type_safe.primitives.domains.cryptography.safe_str.Safe_Str__NaCl__Private_Key import Safe_Str__NaCl__Private_Key

def test_crypto_types(self):
    class Schema__Crypto_Keys(Type_Safe):
        sha1_full    : Safe_Str__SHA1
        nacl_public  : Safe_Str__NaCl__Public_Key
        nacl_private : Safe_Str__NaCl__Private_Key
        
    with Schema__Crypto_Keys() as _:
        # SHA1 must be exactly 40 hex characters
        _.sha1_full = "7fd1a60b01f91b314f59955a4e4d4e80d8edf11d"
        assert len(_.sha1_full) == 40
        
        # Wrong length fails immediately, not during crypto operation
        with pytest.raises(ValueError):
            _.sha1_full = "too-short"
            
        # NaCl keys must be exactly 64 hex characters
        valid_key = "a" * 64
        _.nacl_public = valid_key
        assert len(_.nacl_public) == 64
        
        # These validations prevent:
        # - Cryptographic operation failures
        # - Invalid key storage
        # - Security vulnerabilities from malformed keys
```

### Identifier Safe Types

The new identifier safe_str types provide granular control over different identifier use cases:

```python
from osbot_utils.type_safe.primitives.domains.identifiers.safe_str.Safe_Str__Id           import Safe_Str__Id
from osbot_utils.type_safe.primitives.domains.identifiers.safe_str.Safe_Str__Display_Name import Safe_Str__Display_Name
from osbot_utils.type_safe.primitives.domains.identifiers.safe_str.Safe_Str__Slug         import Safe_Str__Slug
from osbot_utils.type_safe.primitives.domains.identifiers.safe_str.Safe_Str__Topic        import Safe_Str__Topic

def test_identifier_safe_str_types(self):
    class Schema__Service_Registry(Type_Safe):
        service_id   : Safe_Str__Id           # Base identifier
        display_name : Safe_Str__Display_Name # User-facing name
        url_slug     : Safe_Str__Slug         # URL-safe
        category     : Safe_Str__Topic        # Human-readable with spaces
        
    with Schema__Service_Registry() as _:
        # Each type has specific validation rules
        _.service_id = "SVC-123_abc"
        assert _.service_id == "SVC-123_abc"  # Allows alphanumeric, _, -
        
        _.display_name = "My Service (v2.0) #1"
        assert _.display_name == "My Service (v2.0) #1"  # More permissive
        
        # Slugs auto-lowercase
        _.url_slug = "My-Service"  
        assert _.url_slug == "my-service"  # Lowercased automatically
        
        # Topics allow spaces for readability
        _.category = "Cloud Infrastructure Services"
        assert _.category == "Cloud Infrastructure Services"
        
        # Type safety prevents invalid values
        with pytest.raises(ValueError):
            _.url_slug = "Service Name"  # Spaces not allowed in slugs
            
```

### Enum and Literal Support

Type_Safe's Enum and Literal support provides type-safe constants with automatic serialization:

```python
from enum import Enum
from typing import Literal

def test_enum_handling(self):
    class Enum__Status(str, Enum):
        PENDING   = "pending"
        ACTIVE    = "active"
        COMPLETED = "completed"
        
    class Schema__Task(Type_Safe):
        status: Enum__Status = Enum__Status.PENDING
        
    with Schema__Task() as _:
        # String auto-conversion makes APIs easier to use
        _.status = "active"                              # String converts to enum
        assert _.status == Enum__Status.ACTIVE
        assert isinstance(_.status, Enum__Status)
        
        # Invalid values caught immediately
        with pytest.raises(ValueError):
            _.status = "invalid"                         # Not a valid enum value
            
        # JSON serialization preserves enums perfectly
        json_data = _.json()
        assert json_data['status'] == 'active'           # Serializes to string
        
        restored = Schema__Task.from_json(json_data)
        assert restored.status == Enum__Status.ACTIVE    # Restores as enum
        
        # This enables:
        # - Type-safe state machines
        # - API compatibility (strings) with internal type safety (enums)
        # - Prevention of invalid states

def test_literal_validation(self):
    class Schema__Config(Type_Safe):
        environment: Literal["dev", "test", "prod"] = "dev"
        
    with Schema__Config() as _:
        _.environment = "prod"                           # Valid literal
        
        # Literals provide compile-time-like checking at runtime
        with pytest.raises(ValueError):
            _.environment = "staging"                    # Not in literal set
            
        # Use Literal when:
        # - You have a small, fixed set of values
        # - Values won't change often
        # - You don't need enum methods
```

---

## Part 6: Component Testing Patterns

### Testing Type_Safe Schema Classes

Schema classes are the foundation of Type_Safe applications. Testing them thoroughly ensures your data model behaves correctly:

```python
from osbot_utils.type_safe.Type_Safe import Type_Safe
from osbot_utils.type_safe.type_safe_core.collections.Type_Safe__Dict import Type_Safe__Dict
from osbot_utils.type_safe.type_safe_core.collections.Type_Safe__List import Type_Safe__List
from osbot_utils.utils.Objects import base_classes

class test_Schema__Order(TestCase):
    
    def test__init__(self):                              # Always test auto-initialization
        with Schema__Order() as _:
            # First, verify Type_Safe inheritance
            assert type(_)         is Schema__Order
            assert base_classes(_) == [Type_Safe, object]
            
            # Then verify type initialization
            # Collections become Type_Safe variants for safety
            assert type(_.items) is Type_Safe__Dict      # Not regular dict!
            assert type(_.tags)  is Type_Safe__List      # Not regular list!
            
            # Finally, comprehensive state verification
            assert _.obj() == __(id       = _.id         ,  # Auto-generated
                                 items    = {}            ,  # Empty Type_Safe__Dict
                                 tags     = []            ,  # Empty Type_Safe__List
                                 total    = 0.00          ,  # Zero default
                                 status   = "pending"     )  # Explicit default
    
    def test_serialization_round_trip(self):             # Critical for API/storage
        with Schema__Order(id="ORD-123", total=99.99) as original:
            # Serialize to JSON
            json_data = original.json()
            
            # Deserialize back
            with Schema__Order.from_json(json_data) as restored:
                # Must be perfect round-trip
                assert restored.obj() == original.obj()
                
                # Also verify type preservation
                assert type(restored.id)    is Safe_Str__OrderId
                assert type(restored.total) is Safe_Float__Money
```

### Testing Service Classes

Services often have complex initialization chains and external dependencies. Proper testing ensures they work in isolation and integration, and always try to avoid using mocks or patches:

```python
from osbot_utils.utils.Env import get_env
import pytest

class test_Service__OpenRouter(TestCase):
    
    @classmethod
    def setUpClass(cls):
        # Setup expensive resources once
        setup__service_fast_api_test_objs()
        cls.service = Service__OpenRouter()
        
        # Skip if external dependencies missing
        if not get_env(ENV_NAME_OPEN_ROUTER__API_KEY):
            pytest.skip("OpenRouter API key required")
            
    def test_setup(self):                                # Test initialization chain
        with self.service as _:
            # Verify configuration
            assert _.api_base_url == "https://openrouter.ai/api"
            
            # Verify sub-services initialized
            assert type(_.models_service) is Service__OpenRouter__Models
            assert _.s3_storage is not None
            
            # This ensures:
            # - All dependencies are wired correctly
            # - Configuration is loaded properly
            # - Sub-services are initialized
            
    def test_api_call(self):                  # Test making external calls
        with self.service as _:                                    
            result = _.chat_completion("test prompt")
            assert result["choices"][0]["message"]["content"] == "42"            
                
    @pytest.mark.skip(reason="Takes too long, or is not deterministic")       
    def test_paid_model(self):
        # Some LLMs API calls take too long or are not consistent with the output
        # Run manually when needed
        pass
```

### Testing FastAPI Routes

FastAPI route testing requires careful setup to avoid recreating the app for each test:

```python
class test_Routes__LLMs__client(TestCase):
    
    @classmethod
    def setUpClass(cls):
        # Reuse shared FastAPI app and client
        with setup__service_fast_api_test_objs() as _:
            cls.client = _.fast_api__client              
            cls.app = _.fast_api__app
            
            # Set auth headers once
            cls.client.headers[TEST_API_KEY__NAME] = TEST_API_KEY__VALUE
            
    def test__llms__models(self):                        # Test endpoint responses
        response = self.client.get('/llms/models')
        
        # Verify response structure
        assert response.status_code  == 200
        result = response.json()
        assert 'models'              in result
        assert len(result['models'])  > 0
        
        # Verify Type_Safe serialization worked
        for model in result['models']:
            assert 'id'   in model
            assert 'name' in model
            
    def test__error_handling(self):                      # Test error responses
        response = self.client.post('/llms/complete', json={})
        
        assert response.status_code == 422               # Validation error
        # IMPORTANT: Note the Json content alignment in the assert below
        assert response.json()      == { 'detail': [{ 'input': None                ,
                                                      'loc'  : ['query', 'prompt' ],
                                                      'msg'  : 'Field required'    ,
                                                      'type' : 'missing'           }]}
```

---

## Part 7: Bug Documentation Pattern

### Writing Tests for Bugs That Pass

One of the most powerful testing patterns is writing tests for bugs that PASS with the current buggy behavior. This might seem counterintuitive, but it provides enormous value:

1. **Documents the bug** with executable code
2. **Prevents accidental "fixing"** that might break workarounds
3. **Automatically becomes a regression test** when fixed
4. **Preserves institutional knowledge** about issues
5. **Tracks progress** as bugs get fixed over time

```python
class test_Type_Safe__bugs(TestCase):
    
    def test__bug__bool_assigned_to_int(self):          # Clear bug description in name
        """Bug: Type_Safe allows bool values in int fields"""
        
        class An_Class(Type_Safe):
            an_int: int
            
        an_class = An_Class()
        
        # SECTION 1: Document what SHOULD happen
        # This is commented out because it would fail currently
        # with pytest.raises(TypeError):
        #     an_class.an_int = True               # Should reject bool
        # assert an_class.an_int == 0             # Should remain at default
        
        # SECTION 2: Document what ACTUALLY happens (bug)
        # This test PASSES, documenting the buggy behavior
        an_class.an_int = True                           # BUG: accepts bool
        assert an_class.an_int is True                   # BUG: stores bool
        assert type(an_class.an_int) is bool             # BUG: wrong type
        
        # SECTION 3: Document impact/workarounds if needed
        # This bug means code must explicitly check for bool
        # Workaround: Use Safe_Int instead of raw int
```

### The Bug Test Lifecycle

The lifecycle of a bug test follows a predictable pattern:

1. **Bug discovered**: Write passing test documenting current behavior
2. **Bug lives**: Test continues passing, preventing regression
3. **Bug fixed**: Test starts failing (good!)
4. **Test updated**: Uncomment correct behavior, comment/remove buggy behavior
5. **Test moved**: Relocated to regression suite

```python
def test__regression__list__forward_ref__fails_roundtrip(self):
    """Regression: Forward references in lists now work correctly"""
    
    class An_Class(Type_Safe):
        an_list: List['An_Class']
        
    an_class = An_Class(an_list=[An_Class()])
    json_data = an_class.json()
    
    # HISTORICAL BUG (now commented after fix)
    # with pytest.raises(TypeError, match="'ForwardRef' object is not callable"):
    #     An_Class.from_json(json_data)       # Used to fail
    
    # CURRENT BEHAVIOR (was commented during bug phase)
    restored = An_Class.from_json(json_data)
    assert restored.json() == json_data          # Now works!
    
    # This test ensures the bug never returns
```

### Bug Test Organization

Organize bug tests to make them easy to find and manage:

```python
# Active bugs - tests that document current issues
tests/unit/type_safe/
    test_Type_Safe__bugs.py              # Core Type_Safe bugs
    test_Type_Safe__Dict__bugs.py        # Dict-specific bugs
    test_Type_Safe__List__bugs.py        # List-specific bugs
    
# Fixed bugs - regression tests
tests/unit/type_safe/
    test_Type_Safe__regression.py        # Fixed core bugs
    test_Type_Safe__Dict__regression.py  # Fixed Dict bugs
    test_Type_Safe__List__regression.py  # Fixed List bugs
```

This organization:
- Makes it easy to see current issues
- Shows progress over time
- Prevents fixed bugs from returning
- Documents the history of the codebase

---

## Part 8: Error Message Testing

### Why Full Error Messages Matter

Testing complete error messages rather than partial matches is crucial for several reasons:

1. **User Experience**: Error messages are often the only feedback users get
2. **API Contracts**: Changing error messages can break client code
3. **Debugging**: Complete messages provide context
4. **Documentation**: Error messages serve as executable documentation
5. **Regression Prevention**: Catches unintended message changes

```python
import re

def test_validation_errors(self):
    # ✗ AVOID - Partial match can hide important changes
    with pytest.raises(ValueError, match="specific error"):
        self.service.validate("bad_input")
        # What if the message changed to "specific error in field X"?
        # This test would still pass but users see different message
        
    # ✓ PREFERRED - Full message with re.escape
    error_message = "Invalid type for attribute 'user_id'. Expected '<class 'Safe_Id'>' but got '<class 'str'>'"
    with pytest.raises(ValueError, match=re.escape(error_message)):
        self.service.validate("bad_input")
        # Any change to this message will be caught
```

### Testing Multiple Error Scenarios

When testing multiple error conditions, use descriptive variable names to document each scenario:

```python
def test_multiple_validation_errors(self):
    # Each error message variable documents the scenario
    error_missing_field = "Required field 'user_id' is missing from request"
    with pytest.raises(ValueError, match=re.escape(error_missing_field)):
        self.service.process({})
        
    error_invalid_type = "Invalid type for attribute 'age'. Expected '<class 'int'>' but got '<class 'str'>'"
    with pytest.raises(TypeError, match=re.escape(error_invalid_type)):
        self.service.process({'user_id': '123', 'age': 'not_a_number'})
        
    error_out_of_range = "Value 999 exceeds maximum allowed value of 150 for field 'age'"
    with pytest.raises(ValueError, match=re.escape(error_out_of_range)):
        self.service.process({'user_id': '123', 'age': 999})
```

### Special Characters in Error Messages

Many error messages contain regex special characters that must be escaped:

```python
def test_error_with_special_characters(self):
    # Error with parentheses, brackets, dots - common in type descriptions
    error_complex = "Invalid format for field 'config'. Expected Dict[str, Any] but got <class 'list'>"
    with pytest.raises(TypeError, match=re.escape(error_complex)):
        self.service.set_config([1, 2, 3])
        
    # Error with file paths containing dots and slashes
    error_file = "File not found: /path/to/file.json (expected .yaml extension)"
    with pytest.raises(FileNotFoundError, match=re.escape(error_file)):
        self.service.load_config("/path/to/file.json")
        
    # Always use re.escape() to handle these safely
```

---

## Part 9: Safe Type Edge Cases and Validation Patterns

### Testing Safe_Str Regex Modes

Safe_Str types support two fundamentally different regex modes that change their behavior completely:

```python
from osbot_utils.type_safe.primitives.core.Safe_Str import Safe_Str
from osbot_utils.type_safe.primitives.core.enums.Enum__Safe_Str__Regex_Mode import Enum__Safe_Str__Regex_Mode
import re

def test_regex_mode_replace(self):                      # REPLACE mode - sanitization
    class Safe_Str__Sanitizing(Safe_Str):
        regex = re.compile(r'[^a-zA-Z0-9]')            # What to REMOVE
        regex_mode = Enum__Safe_Str__Regex_Mode.REPLACE
        replacement_char = '_'
        
    # REPLACE mode transforms input
    value = Safe_Str__Sanitizing("hello@world.com")
    assert value == "hello_world_com"                   # @ and . replaced with _
    
    # This mode is for:
    # - Sanitizing user input
    # - Creating safe identifiers
    # - Removing dangerous characters

def test_regex_mode_match(self):                        # MATCH mode - validation
    class Safe_Str__Pattern(Safe_Str):
        regex = re.compile(r'^[A-Z]{3}-[0-9]{4}$')     # Pattern to MATCH
        regex_mode = Enum__Safe_Str__Regex_Mode.MATCH
        strict_validation = True                        # Required with MATCH
        
    # MATCH mode validates pattern
    value = Safe_Str__Pattern("ABC-1234")              # Valid
    assert value == "ABC-1234"
    
    with pytest.raises(ValueError):
        Safe_Str__Pattern("abc-1234")                  # Wrong case
        
    # This mode is for:
    # - Validating specific formats
    # - Ensuring data structure
    # - API key patterns
```

Understanding the difference is crucial:
- **REPLACE**: Transforms input to make it safe
- **MATCH**: Validates input matches exact pattern

### Testing Safe_Float Precision

Safe_Float types handle precision differently based on their use case:

```python
from osbot_utils.type_safe.primitives.domains.numerical.safe_float.Safe_Float__Money import Safe_Float__Money

def test_safe_float_precision(self):
    # Safe_Float__Money uses Decimal for exact arithmetic
    with Schema__Invoice() as _:
        _.subtotal = Safe_Float__Money(19.99)
        _.tax = Safe_Float__Money(1.60)
        _.total = _.subtotal + _.tax
        
        # No floating point errors!
        assert _.total == 21.59                         # Exactly 21.59
        assert str(_.total) == "21.59"                 # Not 21.590000000000003
        
    # Safe_Float__Engineering uses epsilon for comparison
    with Schema__Measurement() as _:
        _.temperature = 273.15
        
        # Engineering comparisons use epsilon tolerance
        assert abs(_.temperature - 273.15) < 1e-6
        
        # This prevents:
        # - Financial calculation errors
        # - False test failures from float precision
        # - Rounding issues in scientific calculations
```

### Testing Collection Behavior

Type_Safe collections have special behaviors that must be tested:

```python
from osbot_utils.type_safe.primitives.domains.identifiers.Safe_Id import Safe_Id
from osbot_utils.type_safe.primitives.core.Safe_Int import Safe_Int

def test_collection_type_safety(self):
    class Schema__Test(Type_Safe):
        items: List[Safe_Id]
        data: Dict[str, Safe_Int]
        
    with Schema__Test() as _:
        # Collections enforce type safety on operations
        _.items.append(Safe_Id("ID-1"))                # Valid
        
        # Safe_Id sanitizes content during append
        _.items.append("!raw-string!")
        assert _.items[-1] == '_raw_string_'           # Sanitized
            
        # Dict operations are also type-checked
        _.data["key"] = Safe_Int(42)                   # Valid
        _.data["key2"] = 100                           # Auto-converts to Safe_Int
        
        with pytest.raises(ValueError):                # Note: ValueError, not TypeError
            _.data["key3"] = "not-a-number"            # Cannot convert
            
        # This runtime checking prevents:
        # - Type confusion in collections
        # - Silent data corruption
        # - Hard-to-debug type errors
```

---

## Testing Checklist

### Essential Tests for Every Type_Safe Class

- [ ] `test__init__` verifies Type_Safe inheritance and auto-initialization
- [ ] Test type enforcement with valid and invalid assignments
- [ ] Test serialization round-trip preserves all types including nested objects
- [ ] Test collections become Type_Safe variants (Type_Safe__List, not list)
- [ ] Use `.obj()` for comprehensive state verification
- [ ] Use context managers with `_` throughout
- [ ] Document with inline comments aligned at column 60-80, never docstrings
- [ ] Test both direct Safe type creation AND auto-conversion paths

### Performance and Resource Management

- [ ] Use `setUpClass` for ALL expensive operations (>100ms)
- [ ] Use shared test objects for FastAPI/LocalStack setup
- [ ] Implement proper tearDown cleanup to prevent state pollution
- [ ] Skip tests requiring unavailable API keys with clear messages
- [ ] Verify services can be reused across tests (atomic design)
- [ ] Group related tests using method naming conventions

### Type Safety and Validation

- [ ] Test with both pre-converted Safe types and raw values
- [ ] Understand auto-conversion vs validation vs type errors
- [ ] Test validation boundaries (min/max, length limits)
- [ ] Test full error messages with `re.escape()` for exact matching
- [ ] Test Enum auto-conversion from strings
- [ ] Test Literal value constraints
- [ ] Test regex modes (REPLACE for sanitization vs MATCH for validation)
- [ ] Test Safe_Float precision (Decimal for money, epsilon for engineering)
- [ ] Test identifier Safe_Str types (Safe_Str__Id, Safe_Str__Slug, etc.)
- [ ] Test that Safe_Str__Slug auto-lowercases input
- [ ] Test Safe_Str__Display_Name allows special characters for user-facing text

### Advanced Testing Features

- [ ] Use `__SKIP__` for dynamic values (IDs, timestamps)
- [ ] Use `.contains()` for partial matching in large objects
- [ ] Use `.diff()` for debugging test failures
- [ ] Use `.merge()` for creating test data variations
- [ ] Document bugs with passing tests that show current behavior
- [ ] Test LLM-specific Safe types with size limits
- [ ] Test Git/GitHub format validation
- [ ] Test cryptographic key length requirements

### Code Organization and Maintenance

- [ ] One-to-one mapping between class methods and test methods
- [ ] Use double underscore for test variations (test_method__edge_case)
- [ ] Organize bug tests separately from regression tests
- [ ] Define reusable test data in setUpClass
- [ ] Use descriptive variable names for error messages
- [ ] Follow visual alignment patterns from main Type_Safe guide

---

## Key Testing Philosophy

The philosophy behind Type_Safe testing is that **tests should be as robust as the code they test**. This means:

1. **Type Safety Everywhere**: Tests must validate that Type_Safe's runtime type checking actually works. Never assume type safety - verify it.

2. **Visual Clarity**: Following the alignment patterns makes tests scannable and bugs visible. Misaligned code often indicates actual bugs.

3. **Performance Consciousness**: Fast tests get run more often. Using setUpClass properly can make the difference between a 5-second and 5-minute test suite.

4. **Comprehensive Validation**: The .obj() method and __ class features enable testing complete object state in single assertions, making tests both thorough and maintainable.

5. **Documentation Through Tests**: Tests serve as executable documentation. Bug tests preserve institutional knowledge. Error message tests document API contracts.

6. **Reality Over Idealism**: Type_Safe acknowledges that real-world data is messy. Auto-conversion is a feature, not a bug. Test both the ideal and realistic scenarios.

7. **Fail Fast, Fail Clear**: When tests fail, they should indicate exactly what went wrong. Full error messages, .diff() output, and clear test names make debugging faster.

Remember: This guide works in conjunction with the "Type_Safe & Python Formatting Guide for LLMs". Together, they provide complete coverage for writing Type_Safe code and tests. The goal is not just to test that code works, but to ensure it remains maintainable, performant, and secure as it evolves.